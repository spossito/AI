{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Librerias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX Version: 0.4.34\n",
      "Pytorch Version: 2.5.0+cpu\n"
     ]
    }
   ],
   "source": [
    "# Importamos\n",
    "\n",
    "import jax\n",
    "import torch\n",
    "import jax.numpy as jaxnp\n",
    "\n",
    "from jax import grad\n",
    "\n",
    "# Version \n",
    "\n",
    "print(f'JAX Version: {jax.__version__}')\n",
    "print(f'Pytorch Version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Traza**\n",
    "\n",
    "* **Primer Orden:**\n",
    "\n",
    "$$\\frac{\\partial}{\\partial X} \\, \\text{Tr}(X) = I$$\n",
    "$$\\frac{\\partial}{\\partial X} \\, \\text{Tr}(XA) = A^T$$\n",
    "$$\\frac{\\partial}{\\partial X} \\, \\text{Tr}(AXB) = A^T B^T$$\n",
    "$$\\frac{\\partial}{\\partial X} \\, \\text{Tr}(AX^T B) = BA$$\n",
    "$$\\frac{\\partial}{\\partial X} \\, \\text{Tr}(X^T A) = A$$\n",
    "$$\\frac{\\partial}{\\partial X} \\, \\text{Tr}(AX^T) = A$$\n",
    "$$\\frac{\\partial}{\\partial X} \\, \\text{Tr}(A \\otimes X) = \\text{Tr}(A) I$$\n",
    "\n",
    "* **Segundo Orden:**\n",
    "\n",
    "$$\\frac{\\partial}{\\partial X} \\, \\text{Tr}(X^2) = 2X^T$$\n",
    "$$\\frac{\\partial}{\\partial X} \\, \\text{Tr}(X^2 B) = (XB + BX)^T$$\n",
    "$$\\frac{\\partial}{\\partial X} \\, \\text{Tr}(X^T B X) = BX + B^T X$$\n",
    "$$\\frac{\\partial}{\\partial X} \\, \\text{Tr}(B X X^T) = BX + B^T X$$\n",
    "$$\\frac{\\partial}{\\partial X} \\, \\text{Tr}(X X^T B) = BX + B^T X$$\n",
    "$$\\frac{\\partial}{\\partial X} \\, \\text{Tr}(X B X^T) = X B^T + XB$$\n",
    "$$\\frac{\\partial}{\\partial X} \\, \\text{Tr}(B X^T X) = X B^T + XB$$\n",
    "$$\\frac{\\partial}{\\partial X} \\, \\text{Tr}(X^T X B) = X B^T + XB$$\n",
    "$$\\frac{\\partial}{\\partial X} \\, \\text{Tr}(A X B X) = A^T X^T B^T + B^T X^T A^T$$\n",
    "$$\\frac{\\partial}{\\partial X} \\, \\text{Tr}(X^T X) = \\frac{\\partial}{\\partial X} \\, \\text{Tr}(X X^T) = 2X$$\n",
    "$$\\frac{\\partial}{\\partial X} \\, \\text{Tr}(B^T X^T C X B) = C^T X B B^T + C X B B^T$$\n",
    "$$\\frac{\\partial}{\\partial X} \\, \\text{Tr}(X^T B X C) = B X C + B^T X C^T$$\n",
    "$$\\frac{\\partial}{\\partial X} \\, \\text{Tr}(A X B X^T C) = A^T C^T X B^T + C A X B$$\n",
    "$$\\frac{\\partial}{\\partial X} \\, \\text{Tr} \\big( (A X B + C)(A X B + C)^T \\big) = 2 A^T (A X B + C) B^T$$\n",
    "$$\\frac{\\partial}{\\partial X} \\, \\text{Tr}(X \\otimes X) = \\frac{\\partial}{\\partial X} \\, \\text{Tr}(X) \\text{Tr}(X) = 2 \\, \\text{Tr}(X) I$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio #1**\n",
    "\n",
    "**Funcion:**\n",
    "\n",
    "$$F(X) = \\text{Tr}(XA)$$\n",
    "\n",
    "**Concepto Clave:** \n",
    "\n",
    "$$f'(X) = (\\nabla X)^T$$\n",
    "$$\\nabla X = f'(X)^T$$\n",
    "\n",
    "**Artificios:** \n",
    "\n",
    "$$d(\\text{Tr}(X)) = \\text{Tr}(d(X))$$\n",
    "$$\\text{Tr}(AB) = \\text{Tr}(BA)$$\n",
    "\n",
    "**Derivada:**\n",
    "\n",
    "$$F(X) = \\text{Tr}(AX)$$\n",
    "$$df(X) = \\text{Tr}(d(AX))$$\n",
    "$$df(X) = \\text{Tr}(AdX + dAX)$$\n",
    "$$df(X) = \\text{Tr}(AdX)$$\n",
    "\n",
    "**Gradiente:** \n",
    "\n",
    "$$(\\nabla X)^T = A$$\n",
    "$$\\nabla X = A^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "tensor([[2., 6., 4.],\n",
      "        [8., 1., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# Definimos las Matrices\n",
    "\n",
    "X = torch.tensor([[1.0, 2.0, 7.0], [3.0, 4.0, 9.0]], requires_grad = True)\n",
    "A = torch.tensor([[2.0, 8.0], [6.0, 1.0], [4.0, 3.0]], requires_grad = False)\n",
    "\n",
    "# Definimos Nuestra Funcion Escalar\n",
    "\n",
    "func_torch = torch.trace(torch.matmul(A, X))\n",
    "\n",
    "# Calculamos el Gradiente \n",
    "\n",
    "func_torch.backward()\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{X.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "[[2. 6. 4.]\n",
      " [8. 1. 3.]]\n"
     ]
    }
   ],
   "source": [
    "# Definimos las Matrices\n",
    "\n",
    "X = jaxnp.array([[1.0, 2.0, 7.0], [3.0, 4.0, 9.0]])\n",
    "A = jaxnp.array([[2.0, 8.0], [6.0, 1.0], [4.0, 3.0]])\n",
    "\n",
    "# Definimos Nuestra Funcion Escalar\n",
    "\n",
    "def func_jax(X):\n",
    "    return jaxnp.trace(jaxnp.matmul(A, X))\n",
    "\n",
    "# Calculamos el Gradiente \n",
    "\n",
    "func_grad = grad(fun = func_jax)\n",
    "\n",
    "# Obtenemos el Gradiente con respecto a X \n",
    "\n",
    "X_grad = func_grad(X)\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{X_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "tensor([[2., 6., 4.],\n",
      "        [8., 1., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# Definimos Nuestra Matriz\n",
    "\n",
    "A = torch.tensor([[2.0, 8.0], [6.0, 1.0], [4.0, 3.0]])\n",
    "\n",
    "# Calculamos el Gradiente\n",
    "\n",
    "gradient = A.T\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{gradient}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio #2**\n",
    "\n",
    "**Funcion:**\n",
    "\n",
    "$$F(X) = \\text{Tr}(AXB)$$\n",
    "\n",
    "**Concepto Clave:** \n",
    "\n",
    "$$f'(X) = (\\nabla X)^T$$\n",
    "$$\\nabla X = f'(X)^T$$\n",
    "\n",
    "**Artificios:** \n",
    "\n",
    "$$d(\\text{Tr}(X)) = \\text{Tr}(d(X))$$\n",
    "\n",
    "**Derivada:**\n",
    "\n",
    "$$F(X) = \\text{Tr}(BAX)$$\n",
    "$$df(X) = \\text{Tr}(d(BAX))$$\n",
    "$$df(X) = \\text{Tr}(BAdX + BdAX + dBAX)$$\n",
    "$$df(X) = \\text{Tr}(BAdX)$$\n",
    "\n",
    "**Gradiente:** \n",
    "\n",
    "$$(\\nabla X)^T = BA$$\n",
    "$$\\nabla X = A^TB^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "tensor([[42., 52., 20.],\n",
      "        [57., 21., 44.]])\n"
     ]
    }
   ],
   "source": [
    "# Definimos las Matrices\n",
    "\n",
    "X = torch.tensor([[1.0, 2.0, 7.0], [3.0, 4.0, 9.0]], requires_grad = True)\n",
    "A = torch.tensor([[2.0, 8.0], [6.0, 1.0], [4.0, 3.0]], requires_grad = False)\n",
    "B = torch.tensor([[5.0, 2.0, 5.0], [1.0, 7.0, 2.0], [5.0, 1.0, 1.0]], requires_grad = False)\n",
    "\n",
    "# Definimos Nuestra Funcion Escalar\n",
    "\n",
    "func_torch = torch.trace(torch.matmul(torch.matmul(A, X), B))\n",
    "\n",
    "# Calculamos el Gradiente \n",
    "\n",
    "func_torch.backward()\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{X.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "[[42. 52. 20.]\n",
      " [57. 21. 44.]]\n"
     ]
    }
   ],
   "source": [
    "# Definimos las Matrices\n",
    "\n",
    "X = jaxnp.array([[1.0, 2.0, 7.0], [3.0, 4.0, 9.0]])\n",
    "A = jaxnp.array([[2.0, 8.0], [6.0, 1.0], [4.0, 3.0]])\n",
    "B = jaxnp.array([[5.0, 2.0, 5.0], [1.0, 7.0, 2.0], [5.0, 1.0, 1.0]])\n",
    "\n",
    "# Definimos Nuestra Funcion Escalar\n",
    "\n",
    "def func_jax(X):\n",
    "    return jaxnp.trace(jaxnp.matmul(jaxnp.matmul(A, X), B))\n",
    "\n",
    "# Calculamos el Gradiente \n",
    "\n",
    "func_grad = grad(fun = func_jax)\n",
    "\n",
    "# Obtenemos el Gradiente con respecto a X \n",
    "\n",
    "X_grad = func_grad(X)\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{X_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "tensor([[42., 52., 20.],\n",
      "        [57., 21., 44.]])\n"
     ]
    }
   ],
   "source": [
    "# Definimos Nuestras Matrices\n",
    "\n",
    "A = torch.tensor([[2.0, 8.0], [6.0, 1.0], [4.0, 3.0]])\n",
    "B = torch.tensor([[5.0, 2.0, 5.0], [1.0, 7.0, 2.0], [5.0, 1.0, 1.0]])\n",
    "\n",
    "# Calculamos el Gradiente\n",
    "\n",
    "gradient = torch.matmul(A.T, B.T)\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{gradient}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio #3**\n",
    "\n",
    "**Funcion:**\n",
    "\n",
    "$$F(X) = \\text{Tr}(X^2B)$$\n",
    "\n",
    "**Concepto Clave:** \n",
    "\n",
    "$$f'(X) = (\\nabla X)^T$$\n",
    "$$\\nabla X = f'(X)^T$$\n",
    "\n",
    "**Artificios:** \n",
    "\n",
    "$$d(\\text{Tr}(X)) = \\text{Tr}(d(X))$$\n",
    "$$\\text{Tr}(ABC) = \\text{Tr}(CAB) = \\text{Tr}(BCA)$$\n",
    "$$\\text{Tr}(A + B) = \\text{Tr}(A) + \\text{Tr}(B)$$\n",
    "\n",
    "**Derivada:**\n",
    "\n",
    "$$F(X) = \\text{Tr}(XXB)$$\n",
    "$$F(X) = \\text{Tr}(BXX)$$\n",
    "$$df(X) = \\text{Tr}(d(BXX))$$\n",
    "$$df(X) = \\text{Tr}(BXdX + BdXX + dBXX)$$\n",
    "$$df(X) = \\text{Tr}(BXdX + BdXX)$$\n",
    "$$df(X) = \\text{Tr}(BXdX) +  \\text{Tr}(BdXX)$$\n",
    "$$df(X) = \\text{Tr}(BXdX) +  \\text{Tr}(XBdX)$$\n",
    "\n",
    "**Gradiente:** \n",
    "\n",
    "$$(\\nabla X)^T = BX + XB$$\n",
    "$$\\nabla X = (BX + XB)^T$$\n",
    "\n",
    "---------------\n",
    "\n",
    "**NOTA:** Si definimos dentro de Pytorch y JAX la funcion $\\text{trace}(\\text{matmul}(X^2, B))$ el calculo del gradiente sale errado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "tensor([[ 78.,  96.,  47.],\n",
      "        [ 61.,  81.,  57.],\n",
      "        [ 74., 104.,  79.]])\n"
     ]
    }
   ],
   "source": [
    "# Definimos las Matrices\n",
    "\n",
    "X = torch.tensor([[1.0, 2.0, 7.0], [3.0, 4.0, 9.0], [5.0, 4.0, 1.0]], requires_grad = True)\n",
    "B = torch.tensor([[5.0, 2.0, 5.0], [1.0, 7.0, 2.0], [5.0, 1.0, 1.0]], requires_grad = False)\n",
    "\n",
    "# Definimos Nuestra Funcion Escalar\n",
    "\n",
    "func_torch = torch.trace(torch.matmul(torch.matmul(X, X), B))\n",
    "\n",
    "# Calculamos el Gradiente \n",
    "\n",
    "func_torch.backward()\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{X.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "[[ 78.  96.  47.]\n",
      " [ 61.  81.  57.]\n",
      " [ 74. 104.  79.]]\n"
     ]
    }
   ],
   "source": [
    "# Definimos las Matrices\n",
    "\n",
    "X = jaxnp.array([[1.0, 2.0, 7.0], [3.0, 4.0, 9.0], [5.0, 4.0, 1.0]])\n",
    "B = jaxnp.array([[5.0, 2.0, 5.0], [1.0, 7.0, 2.0], [5.0, 1.0, 1.0]])\n",
    "\n",
    "# Definimos Nuestra Funcion Escalar\n",
    "\n",
    "def func_jax(X):\n",
    "    return jaxnp.trace(jaxnp.matmul(jaxnp.matmul(X, X), B))\n",
    "\n",
    "# Calculamos el Gradiente \n",
    "\n",
    "func_grad = grad(fun = func_jax)\n",
    "\n",
    "# Obtenemos el Gradiente con respecto a X \n",
    "\n",
    "X_grad = func_grad(X)\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{X_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "tensor([[ 78.,  96.,  47.],\n",
      "        [ 61.,  81.,  57.],\n",
      "        [ 74., 104.,  79.]])\n"
     ]
    }
   ],
   "source": [
    "# Definimos Nuestras Matrices\n",
    "\n",
    "X = torch.tensor([[1.0, 2.0, 7.0], [3.0, 4.0, 9.0], [5.0, 4.0, 1.0]])\n",
    "B = torch.tensor([[5.0, 2.0, 5.0], [1.0, 7.0, 2.0], [5.0, 1.0, 1.0]])\n",
    "\n",
    "# Calculamos el Gradiente\n",
    "\n",
    "gradient = (torch.matmul(B, X) + torch.matmul(X, B)).T\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{gradient}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio #4**\n",
    "\n",
    "**Funcion:**\n",
    "\n",
    "$$F(X) = \\text{Tr}(AXBX^TC)$$\n",
    "\n",
    "**Concepto Clave:** \n",
    "\n",
    "$$f'(X) = (\\nabla X)^T$$\n",
    "$$\\nabla X = f'(X)^T$$\n",
    "\n",
    "**Artificios:** \n",
    "\n",
    "$$d(\\text{Tr}(X)) = \\text{Tr}(d(X))$$\n",
    "$$\\text{Tr}(ABC) = \\text{Tr}(CAB) = \\text{Tr}(BCA)$$\n",
    "$$\\text{Tr}(A + B) = \\text{Tr}(A) + \\text{Tr}(B)$$\n",
    "\n",
    "**Derivada:**\n",
    "\n",
    "$$df(X) = \\text{Tr}(d(AXBX^TC))$$\n",
    "$$df(X) = \\text{Tr}(AXBX^TdC + AXBdX^TC + AXdBX^TC + AdXBX^TC + dAXBX^TC)$$\n",
    "$$df(X) = \\text{Tr}(AXBdX^TC + AdXBX^TC)$$\n",
    "$$df(X) = \\text{Tr}(AXBdX^TC) +  \\text{Tr}(AdXBX^TC)$$\n",
    "$$df(X) = \\text{Tr}(dX^TCAXB) +  \\text{Tr}(BX^TCAdX)$$\n",
    "$$df(X) = \\text{Tr}(dX^TCAXB) +  \\text{Tr}(BX^TCAdX)$$\n",
    "$$df(X) = \\text{Tr}(B^TX^TA^TC^TdX) +  \\text{Tr}(BX^TCAdX)$$\n",
    "**Gradiente:** \n",
    "\n",
    "$$(\\nabla X)^T = B^TX^TA^TC^T + BX^TCA$$\n",
    "$$\\nabla X = CAXB + A^TC^TXB^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "tensor([[12612.,  9502.,  7804., 18310.],\n",
      "        [10815.,  8734.,  7179., 11827.],\n",
      "        [16249., 11958.,  9793., 25043.]])\n"
     ]
    }
   ],
   "source": [
    "# Definimos las Matrices\n",
    "\n",
    "X = torch.tensor([[1.0, 2.0, 7.0, 5.0], [3.0, 4.0, 9.0, 1.0], [5.0, 4.0, 1.0, 3.0]], requires_grad = True)\n",
    "A = torch.tensor([[5.0, 2.0, 5.0], [1.0, 7.0, 2.0]], requires_grad = False)\n",
    "B = torch.tensor([[5.0, 2.0, 5.0, 9.0], [1.0, 7.0, 2.0, 9.0], [5.0, 1.0, 1.0, 9.0], [5.0, 1.0, 1.0, 9.0]], requires_grad = False)\n",
    "C = torch.tensor([[5.0, 4.0], [2.0, 2.0], [8.0, 5.0]], requires_grad = False)\n",
    "\n",
    "# Definimos Nuestra Funcion Escalar\n",
    "\n",
    "func_torch = torch.trace(torch.matmul(torch.matmul(torch.matmul(torch.matmul(A, X), B), X.T), C))\n",
    "\n",
    "# Calculamos el Gradiente \n",
    "\n",
    "func_torch.backward()\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{X.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "[[12612.  9502.  7804. 18310.]\n",
      " [10815.  8734.  7179. 11827.]\n",
      " [16249. 11958.  9793. 25043.]]\n"
     ]
    }
   ],
   "source": [
    "# Definimos las Matrices\n",
    "\n",
    "X = jaxnp.array([[1.0, 2.0, 7.0, 5.0], [3.0, 4.0, 9.0, 1.0], [5.0, 4.0, 1.0, 3.0]])\n",
    "A = jaxnp.array([[5.0, 2.0, 5.0], [1.0, 7.0, 2.0]])\n",
    "B = jaxnp.array([[5.0, 2.0, 5.0, 9.0], [1.0, 7.0, 2.0, 9.0], [5.0, 1.0, 1.0, 9.0], [5.0, 1.0, 1.0, 9.0]])\n",
    "C = jaxnp.array([[5.0, 4.0], [2.0, 2.0], [8.0, 5.0]])\n",
    "\n",
    "# Definimos Nuestra Funcion Escalar\n",
    "\n",
    "def func_jax(X):\n",
    "    return jaxnp.trace(jaxnp.matmul(jaxnp.matmul(jaxnp.matmul(jaxnp.matmul(A, X), B), X.T), C))\n",
    "\n",
    "# Calculamos el Gradiente \n",
    "\n",
    "func_grad = grad(fun = func_jax)\n",
    "\n",
    "# Obtenemos el Gradiente con respecto a X \n",
    "\n",
    "X_grad = func_grad(X)\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{X_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "tensor([[12612.,  9502.,  7804., 18310.],\n",
      "        [10815.,  8734.,  7179., 11827.],\n",
      "        [16249., 11958.,  9793., 25043.]])\n"
     ]
    }
   ],
   "source": [
    "# Definimos Nuestras Matrices\n",
    "\n",
    "X = torch.tensor([[1.0, 2.0, 7.0, 5.0], [3.0, 4.0, 9.0, 1.0], [5.0, 4.0, 1.0, 3.0]])\n",
    "A = torch.tensor([[5.0, 2.0, 5.0], [1.0, 7.0, 2.0]])\n",
    "B = torch.tensor([[5.0, 2.0, 5.0, 9.0], [1.0, 7.0, 2.0, 9.0], [5.0, 1.0, 1.0, 9.0], [5.0, 1.0, 1.0, 9.0]])\n",
    "C = torch.tensor([[5.0, 4.0], [2.0, 2.0], [8.0, 5.0]])\n",
    "\n",
    "# Calculamos el Gradiente\n",
    "\n",
    "gradient = torch.matmul(torch.matmul(torch.matmul(C, A), X), B) + torch.matmul(torch.matmul(torch.matmul(A.T, C.T), X), B.T)\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{gradient}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Norma**\n",
    "\n",
    "**Frobenius:**\n",
    "\n",
    "$$\\frac{\\partial}{\\partial X} \\, \\|X\\|_F^2 = \\frac{\\partial}{\\partial X} \\, \\text{Tr}(X X^T)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio #1**\n",
    "\n",
    "**Funcion:**\n",
    "\n",
    "$$F(X) = \\, \\|X\\|_F^2$$\n",
    "\n",
    "**Concepto Clave:** \n",
    "\n",
    "$$f'(X) = (\\nabla X)^T$$\n",
    "$$\\nabla X = f'(X)^T$$\n",
    "\n",
    "**Artificios:** \n",
    "\n",
    "$$\\, \\|X\\|_F^2 = \\text{Tr}(X X^T)$$\n",
    "$$d(\\text{Tr}(X)) = \\text{Tr}(d(X))$$\n",
    "$$\\text{Tr}(ABC) = \\text{Tr}(CAB) = \\text{Tr}(BCA)$$\n",
    "$$\\text{Tr}(A + B) = \\text{Tr}(A) + \\text{Tr}(B)$$\n",
    "\n",
    "**Derivada:**\n",
    "\n",
    "$$df(X) = \\text{Tr}(d(X X^T))$$\n",
    "$$df(X) = \\text{Tr}(XdX^T + dXX^T)$$\n",
    "$$df(X) = \\text{Tr}(dXX^T + dXX^T)$$\n",
    "$$df(X) = \\text{Tr}(dXX^T) +  \\text{Tr}(dXX^T)$$\n",
    "$$df(X) = \\text{Tr}(X^TdX) +  \\text{Tr}(X^TdX)$$\n",
    "\n",
    "**Gradiente:** \n",
    "\n",
    "$$(\\nabla X)^T = X^T + X^T = 2X^T$$\n",
    "$$\\nabla X = 2X$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "tensor([[ 2.,  4., 14., 10.],\n",
      "        [ 6.,  8., 18.,  2.],\n",
      "        [10.,  8.,  2.,  6.],\n",
      "        [18., 10., 14.,  4.]])\n"
     ]
    }
   ],
   "source": [
    "# Definimos Nuestra Matriz\n",
    "\n",
    "X = torch.tensor([[1.0, 2.0, 7.0, 5.0], [3.0, 4.0, 9.0, 1.0], [5.0, 4.0, 1.0, 3.0], [9.0, 5.0, 7.0, 2.0]], requires_grad = True)\n",
    "\n",
    "# Definimos Nuestra Funcion Escalar\n",
    "\n",
    "func_torch = torch.norm(X)**2\n",
    "\n",
    "# Calculamos el Gradiente \n",
    "\n",
    "func_torch.backward()\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{X.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "[[ 2.  4. 14. 10.]\n",
      " [ 6.  8. 18.  2.]\n",
      " [10.  8.  2.  6.]\n",
      " [18. 10. 14.  4.]]\n"
     ]
    }
   ],
   "source": [
    "# Definimos Nuestra Matriz\n",
    "\n",
    "X = jaxnp.array([[1.0, 2.0, 7.0, 5.0], [3.0, 4.0, 9.0, 1.0], [5.0, 4.0, 1.0, 3.0], [9.0, 5.0, 7.0, 2.0]])\n",
    "\n",
    "# Definimos Nuestra Funcion Escalar\n",
    "\n",
    "def func_jax(X):\n",
    "    return jaxnp.linalg.norm(X)**2\n",
    "\n",
    "# Calculamos el Gradiente \n",
    "\n",
    "func_grad = grad(fun = func_jax)\n",
    "\n",
    "# Obtenemos el Gradiente con respecto a X \n",
    "\n",
    "X_grad = func_grad(X)\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{X_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "tensor([[ 2.,  4., 14., 10.],\n",
      "        [ 6.,  8., 18.,  2.],\n",
      "        [10.,  8.,  2.,  6.],\n",
      "        [18., 10., 14.,  4.]])\n"
     ]
    }
   ],
   "source": [
    "# Definimos Nuestra Matriz\n",
    "\n",
    "X = torch.tensor([[1.0, 2.0, 7.0, 5.0], [3.0, 4.0, 9.0, 1.0], [5.0, 4.0, 1.0, 3.0], [9.0, 5.0, 7.0, 2.0]])\n",
    "\n",
    "# Calculamos el Gradiente\n",
    "\n",
    "gradient = 2 * X\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{gradient}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio #2**\n",
    "\n",
    "**Funcion:**\n",
    "\n",
    "$$F(X) = ||AX - B||^{2}$$\n",
    "\n",
    "**Concepto Clave:** \n",
    "\n",
    "$$f'(X) = (\\nabla X)^T$$\n",
    "$$\\nabla X = f'(X)^T$$\n",
    "\n",
    "**Artificios:** \n",
    "\n",
    "$$\\, \\|X\\|_F^2 = \\text{Tr}(X X^T)$$\n",
    "$$d(\\text{Tr}(X)) = \\text{Tr}(d(X))$$\n",
    "$$\\text{Tr}(ABC) = \\text{Tr}(CAB) = \\text{Tr}(BCA)$$\n",
    "$$\\text{Tr}(A + B) = \\text{Tr}(A) + \\text{Tr}(B)$$\n",
    "\n",
    "**Derivada:**\n",
    "\n",
    "$$df(X) = \\text{Tr}(d((AX - B) (AX - B)^T))$$\n",
    "$$df(X) = \\text{Tr}((AX - B) d(AX - B)^T + d(AX - B) (AX - B)^T)$$\n",
    "$$df(X) = \\text{Tr}((AX - B) dX^TA^T + AdX (AX - B)^T)$$\n",
    "$$df(X) = \\text{Tr}(AdX(AX - B)^T + AdX (AX - B)^T)$$\n",
    "$$df(X) = \\text{Tr}(AdX(AX - B)^T) +  \\text{Tr}(AdX (AX - B)^T)$$\n",
    "$$df(X) = \\text{Tr}((AX - B)^TAdX) +  \\text{Tr}((AX - B)^TAdX)$$\n",
    "\n",
    "**Gradiente:** \n",
    "\n",
    "$$(\\nabla X)^T = (AX - B)^TA + (AX - B)^TA = 2(AX - B)^TA $$\n",
    "$$\\nabla X = 2  A^T (AX - B)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "tensor([[ 504.,  964., 2180.],\n",
      "        [ 612., 1112., 2498.]])\n"
     ]
    }
   ],
   "source": [
    "# Definimos Nuestra Matrices\n",
    "\n",
    "X = torch.tensor([[1.0, 2.0, 6.0], [3.0, 4.0, 7.0]], requires_grad = True)\n",
    "A = torch.tensor([[1.0, 2.0], [3.0, 4.0], [7.0, 9.0], [5.0, 3.0]], requires_grad = False)\n",
    "B = torch.tensor([[1.0, 2.0, 6.0], [3.0, 4.0, 7.0], [9.0, 5.0, 3.0], [7.0, 1.0, 2.0]], requires_grad = False)\n",
    "\n",
    "# Definimos Nuestra Funcion Escalar\n",
    "\n",
    "func_torch = torch.norm(torch.matmul(A, X) - B)**2\n",
    "\n",
    "# Calculamos el Gradiente \n",
    "\n",
    "func_torch.backward()\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{X.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "[[ 504.  964. 2180.]\n",
      " [ 612. 1112. 2498.]]\n"
     ]
    }
   ],
   "source": [
    "# Definimos Nuestras Matrices\n",
    "\n",
    "X = jaxnp.array([[1.0, 2.0, 6.0], [3.0, 4.0, 7.0]])\n",
    "A = jaxnp.array([[1.0, 2.0], [3.0, 4.0], [7.0, 9.0], [5.0, 3.0]])\n",
    "B = jaxnp.array([[1.0, 2.0, 6.0], [3.0, 4.0, 7.0], [9.0, 5.0, 3.0], [7.0, 1.0, 2.0]])\n",
    "\n",
    "# Definimos Nuestra Funcion Escalar\n",
    "\n",
    "def func_jax(X):\n",
    "    return jaxnp.linalg.norm(jaxnp.matmul(A, X) - B)**2\n",
    "\n",
    "# Calculamos el Gradiente \n",
    "\n",
    "func_grad = grad(fun = func_jax)\n",
    "\n",
    "# Obtenemos el Gradiente con respecto a X \n",
    "\n",
    "X_grad = func_grad(X)\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{X_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "tensor([[ 504.,  964., 2180.],\n",
      "        [ 612., 1112., 2498.]])\n"
     ]
    }
   ],
   "source": [
    "# Definimos Nuestra Matrices\n",
    "\n",
    "X = torch.tensor([[1.0, 2.0, 6.0], [3.0, 4.0, 7.0]])\n",
    "A = torch.tensor([[1.0, 2.0], [3.0, 4.0], [7.0, 9.0], [5.0, 3.0]])\n",
    "B = torch.tensor([[1.0, 2.0, 6.0], [3.0, 4.0, 7.0], [9.0, 5.0, 3.0], [7.0, 1.0, 2.0]])\n",
    "\n",
    "# Calculamos el Gradiente\n",
    "\n",
    "gradient = 2 * torch.matmul(A.T, (torch.matmul(A, X) - B))\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{gradient}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio #3**\n",
    "\n",
    "**Funcion:**\n",
    "\n",
    "$$F(X) = ||X||_F$$\n",
    "\n",
    "**Concepto Clave:** \n",
    "\n",
    "$$f'(X) = (\\nabla X)^T$$\n",
    "$$\\nabla X = f'(X)^T$$\n",
    "\n",
    "**Artificios:** \n",
    "\n",
    "$$\\, \\|X\\|_F^2 = \\text{Tr}(X X^T)$$\n",
    "$$d(\\text{Tr}(X)) = \\text{Tr}(d(X))$$\n",
    "$$\\text{Tr}(ABC) = \\text{Tr}(CAB) = \\text{Tr}(BCA)$$\n",
    "$$\\text{Tr}(A + B) = \\text{Tr}(A) + \\text{Tr}(B)$$\n",
    "\n",
    "**Derivada:**\n",
    "\n",
    "$$df(X) = \\sqrt{\\text{Tr}(X X^T)}$$\n",
    "$$df(X) = \\frac{1}{2 \\sqrt{\\text{Tr}(X X^T)}} \\cdot d(\\text{Tr}(X X^T))$$\n",
    "$$df(X) = \\frac{1}{2 ||X||_F} \\cdot \\text{Tr}(X dX^T + dX X^T)$$\n",
    "$$df(X) = \\frac{1}{2 ||X||_F} \\cdot \\text{Tr}(dX X^T + dX X^T)$$\n",
    "$$df(X) = \\frac{1}{2 ||X||_F} \\cdot \\text{Tr}(dX X^T) +  \\text{Tr}(dX X^T)$$\n",
    "$$df(X) = \\frac{1}{2 ||X||_F} \\cdot \\text{Tr}(X^T dX) +  \\text{Tr}(X^T dX)$$\n",
    "$$df(X) = \\frac{1}{2 ||X||_F} \\cdot 2 \\cdot \\text{Tr}(X^T dX)$$\n",
    "$$df(X) = \\frac{1}{||X||_F} \\cdot \\text{Tr}(X^T dX)$$\n",
    "\n",
    "**Gradiente:** \n",
    "\n",
    "$$(\\nabla X)^T = \\frac{X^T}{||X||_F} $$\n",
    "$$\\nabla X = \\frac{X}{||X||_F}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "tensor([[0.0685, 0.1370, 0.4111],\n",
      "        [0.2056, 0.2741, 0.4796],\n",
      "        [0.3426, 0.2056, 0.5482]])\n"
     ]
    }
   ],
   "source": [
    "# Definimos Nuestra Matriz\n",
    "\n",
    "X = torch.tensor([[1.0, 2.0, 6.0], [3.0, 4.0, 7.0], [5.0, 3.0, 8.0]], requires_grad = True)\n",
    "\n",
    "# Definimos Nuestra Funcion Escalar\n",
    "\n",
    "func_torch = torch.norm(X)\n",
    "\n",
    "# Calculamos el Gradiente \n",
    "\n",
    "func_torch.backward()\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{X.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "[[0.06851887 0.13703774 0.4111132 ]\n",
      " [0.2055566  0.27407548 0.47963208]\n",
      " [0.34259436 0.2055566  0.54815096]]\n"
     ]
    }
   ],
   "source": [
    "# Definimos Nuestra Matriz\n",
    "\n",
    "X = jaxnp.array([[1.0, 2.0, 6.0], [3.0, 4.0, 7.0], [5.0, 3.0, 8.0]])\n",
    "\n",
    "# Definimos Nuestra Funcion Escalar\n",
    "\n",
    "def func_jax(X):\n",
    "    return jaxnp.linalg.norm(X)\n",
    "\n",
    "# Calculamos el Gradiente \n",
    "\n",
    "func_grad = grad(fun = func_jax)\n",
    "\n",
    "# Obtenemos el Gradiente con respecto a X \n",
    "\n",
    "X_grad = func_grad(X)\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{X_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "tensor([[0.0685, 0.1370, 0.4111],\n",
      "        [0.2056, 0.2741, 0.4796],\n",
      "        [0.3426, 0.2056, 0.5482]])\n"
     ]
    }
   ],
   "source": [
    "# Definimos Nuestra Matriz\n",
    "\n",
    "X = torch.tensor([[1.0, 2.0, 6.0], [3.0, 4.0, 7.0], [5.0, 3.0, 8.0]])\n",
    "\n",
    "# Calculamos el Gradiente\n",
    "\n",
    "gradient = X / torch.norm(X)\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{gradient}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Determinante**\n",
    "\n",
    "$$\\frac{\\partial \\, \\det(X)}{\\partial X} = \\det(X) \\, \\text{Tr}(X^{-1})$$\n",
    "$$\\frac{\\partial \\, \\ln(\\det(X))}{\\partial X} = \\text{Tr}(X^{-1})$$\n",
    "$$\\frac{\\partial \\, \\det(AX B)}{\\partial X} = \\det(AX B) \\, \\text{Tr}(X^{-1})^T$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio #1**\n",
    "\n",
    "**Funcion:**\n",
    "\n",
    "$$F(X) = \\ln(\\det(X))$$\n",
    "\n",
    "**Concepto Clave:** \n",
    "\n",
    "$$f'(X) = (\\nabla X)^T$$\n",
    "$$\\nabla X = f'(X)^T$$\n",
    "\n",
    "**Artificios:** \n",
    "\n",
    "$$\\ln(\\det(X)) = {\\text{Tr}(\\ln(X))}$$\n",
    "$$d(\\text{Tr}(X)) = \\text{Tr}(d(X))$$\n",
    "\n",
    "**Derivada:**\n",
    "\n",
    "$$df(X) = \\text{Tr}(\\ln(X))$$\n",
    "$$df(X) = \\text{Tr}(d\\ln(X))$$\n",
    "$$df(X) = \\text{Tr}(\\frac{1}{X} \\cdot dX)$$\n",
    "$$df(X) = \\text{Tr}(X^{-1} dX)$$\n",
    "\n",
    "**Gradiente:** \n",
    "\n",
    "$$(\\nabla X)^T = X^{-1} $$\n",
    "$$\\nabla X = (X^{-1})^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "tensor([[-0.3333, -0.3333,  0.3333],\n",
      "        [-0.0606,  0.6667, -0.2121],\n",
      "        [ 0.3030, -0.3333,  0.0606]])\n"
     ]
    }
   ],
   "source": [
    "# Definimos Nuestra Matriz\n",
    "\n",
    "X = torch.tensor([[1.0, 2.0, 6.0], [3.0, 4.0, 7.0], [5.0, 3.0, 8.0]], requires_grad = True)\n",
    "\n",
    "# Definimos Nuestra Funcion Escalar\n",
    "\n",
    "func_torch = torch.log(torch.det(X))\n",
    "\n",
    "# Calculamos el Gradiente \n",
    "\n",
    "func_torch.backward()\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{X.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "[[-0.33333337 -0.33333337  0.33333334]\n",
      " [-0.06060606  0.6666667  -0.21212122]\n",
      " [ 0.3030303  -0.33333334  0.06060606]]\n"
     ]
    }
   ],
   "source": [
    "# Definimos Nuestra Matriz\n",
    "\n",
    "X = jaxnp.array([[1.0, 2.0, 6.0], [3.0, 4.0, 7.0], [5.0, 3.0, 8.0]])\n",
    "\n",
    "# Definimos Nuestra Funcion Escalar\n",
    "\n",
    "def func_jax(X):\n",
    "    return jaxnp.log(jaxnp.linalg.det(X))\n",
    "\n",
    "# Calculamos el Gradiente \n",
    "\n",
    "func_grad = grad(fun = func_jax)\n",
    "\n",
    "# Obtenemos el Gradiente con respecto a X \n",
    "\n",
    "X_grad = func_grad(X)\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{X_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "tensor([[-0.3333, -0.3333,  0.3333],\n",
      "        [-0.0606,  0.6667, -0.2121],\n",
      "        [ 0.3030, -0.3333,  0.0606]])\n"
     ]
    }
   ],
   "source": [
    "# Definimos Nuestra Matriz\n",
    "\n",
    "X = torch.tensor([[1.0, 2.0, 6.0], [3.0, 4.0, 7.0], [5.0, 3.0, 8.0]])\n",
    "\n",
    "# Calculamos el Gradiente\n",
    "\n",
    "gradient = torch.linalg.inv(X).T\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{gradient}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio #2**\n",
    "\n",
    "**Funcion:**\n",
    "\n",
    "$$F(X) = \\det(X)$$\n",
    "\n",
    "**Concepto Clave:** \n",
    "\n",
    "$$f'(X) = (\\nabla X)^T$$\n",
    "$$\\nabla X = f'(X)^T$$\n",
    "\n",
    "**Artificios:** \n",
    "\n",
    "$$\\det(X) = e^{\\text{Tr}(\\ln(X))}$$\n",
    "$$d(\\text{Tr}(X)) = \\text{Tr}(d(X))$$\n",
    "\n",
    "**Derivada:**\n",
    "\n",
    "$$df(X) = e^{\\text{Tr}(\\ln(X))}$$\n",
    "$$df(X) = e^{\\text{Tr}(\\ln(X))} \\cdot \\text{Tr}(d\\ln(X))$$\n",
    "$$df(X) = e^{\\text{Tr}(\\ln(X))} \\cdot \\text{Tr}(\\frac{1}{X} \\cdot dX)$$\n",
    "$$df(X) = e^{\\text{Tr}(\\ln(X))} \\cdot \\text{Tr}(X^{-1} dX)$$\n",
    "$$df(X) = \\det(X) \\cdot \\text{Tr}(X^{-1} dX)$$\n",
    "\n",
    "**Gradiente:** \n",
    "\n",
    "$$(\\nabla X)^T = \\det(X) \\cdot X^{-1} $$\n",
    "$$\\nabla X = \\det(X) \\cdot (X^{-1})^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "tensor([[ 11.0000,  11.0000, -11.0000],\n",
      "        [  2.0000, -22.0000,   7.0000],\n",
      "        [-10.0000,  11.0000,  -2.0000]])\n"
     ]
    }
   ],
   "source": [
    "# Definimos Nuestra Matriz\n",
    "\n",
    "X = torch.tensor([[1.0, 2.0, 6.0], [3.0, 4.0, 7.0], [5.0, 3.0, 8.0]], requires_grad = True)\n",
    "\n",
    "# Definimos Nuestra Funcion Escalar\n",
    "\n",
    "func_torch = torch.det(X)\n",
    "\n",
    "# Calculamos el Gradiente \n",
    "\n",
    "func_torch.backward()\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{X.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "[[ 11.  11. -11.]\n",
      " [  2. -22.   7.]\n",
      " [-10.  11.  -2.]]\n"
     ]
    }
   ],
   "source": [
    "# Definimos Nuestra Matriz\n",
    "\n",
    "X = jaxnp.array([[1.0, 2.0, 6.0], [3.0, 4.0, 7.0], [5.0, 3.0, 8.0]])\n",
    "\n",
    "# Definimos Nuestra Funcion Escalar\n",
    "\n",
    "def func_jax(X):\n",
    "    return jaxnp.linalg.det(X)\n",
    "\n",
    "# Calculamos el Gradiente \n",
    "\n",
    "func_grad = grad(fun = func_jax)\n",
    "\n",
    "# Obtenemos el Gradiente con respecto a X \n",
    "\n",
    "X_grad = func_grad(X)\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{X_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "tensor([[ 11.0000,  11.0000, -11.0000],\n",
      "        [  2.0000, -22.0000,   7.0000],\n",
      "        [-10.0000,  11.0000,  -2.0000]])\n"
     ]
    }
   ],
   "source": [
    "# Definimos Nuestra Matriz\n",
    "\n",
    "X = torch.tensor([[1.0, 2.0, 6.0], [3.0, 4.0, 7.0], [5.0, 3.0, 8.0]])\n",
    "\n",
    "# Calculamos el Gradiente\n",
    "\n",
    "gradient = torch.linalg.det(X) * torch.linalg.inv(X).T\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{gradient}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio #3**\n",
    "\n",
    "**Funcion:**\n",
    "\n",
    "$$F(X) = \\det(AXB)$$\n",
    "\n",
    "**Concepto Clave:** \n",
    "\n",
    "$$f'(X) = (\\nabla X)^T$$\n",
    "$$\\nabla X = f'(X)^T$$\n",
    "\n",
    "**Artificios:** \n",
    "\n",
    "$$\\det(X) = e^{\\text{Tr}(\\ln(X))}$$\n",
    "$$d(\\text{Tr}(X)) = \\text{Tr}(d(X))$$\n",
    "$$\\text{Tr}(ABC) = \\text{Tr}(CAB) = \\text{Tr}(BCA)$$\n",
    "\n",
    "**Derivada:**\n",
    "\n",
    "$$df(X) = e^{\\text{Tr}(\\ln(AXB))}$$\n",
    "$$df(X) = e^{\\text{Tr}(\\ln(AXB))} \\cdot \\text{Tr}(d\\ln(AXB))$$\n",
    "$$df(X) = e^{\\text{Tr}(\\ln(AXB))} \\cdot \\text{Tr}(\\frac{1}{AXB} \\cdot d(AXB))$$\n",
    "$$df(X) = e^{\\text{Tr}(\\ln(AXB))} \\cdot \\text{Tr}(\\frac{1}{AXB} \\cdot d(BAX))$$\n",
    "$$df(X) = e^{\\text{Tr}(\\ln(AXB))} \\cdot \\text{Tr}(\\frac{1}{AXB} \\cdot (BAdX + BdAX + dBAX))$$\n",
    "$$df(X) = e^{\\text{Tr}(\\ln(AXB))} \\cdot \\text{Tr}(\\frac{1}{AXB} \\cdot BAdX)$$\n",
    "$$df(X) = e^{\\text{Tr}(\\ln(AXB))} \\cdot \\text{Tr}(\\frac{1}{X} \\cdot dX)$$\n",
    "$$df(X) = \\det(AXB) \\cdot \\text{Tr}(X^{-1} dX)$$\n",
    "\n",
    "**Gradiente:** \n",
    "\n",
    "$$(\\nabla X)^T = \\det(AXB) \\cdot X^{-1} $$\n",
    "$$\\nabla X = \\det(AXB) \\cdot (X^{-1})^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "tensor([[ 11731.5000, -70388.9844,  27373.4941],\n",
      "        [-15642.0000,  62567.9766, -15641.9951],\n",
      "        [ 19552.5000, -23463.0176,   3910.5015]])\n"
     ]
    }
   ],
   "source": [
    "# Definimos Nuestra Matrices\n",
    "\n",
    "X = torch.tensor([[1.0, 2.0, 7.0], [3.0, 4.0, 9.0], [5.0, 2.0, 3.0]], requires_grad = True)\n",
    "A = torch.tensor([[2.0, 1.0, 0.5], [1.0, 3.0, 1.0], [0.5, 1.0, 2.0]], requires_grad = False)\n",
    "B = torch.tensor([[5.0, 4.0, 5.0], [8.0, 7.0, 2.0], [10.0, 1.0, 1.0]], requires_grad = False)\n",
    "\n",
    "# Definimos Nuestra Funcion Escalar\n",
    "\n",
    "func_torch = torch.det(torch.matmul(torch.matmul(A, X), B))\n",
    "\n",
    "# Calculamos el Gradiente \n",
    "\n",
    "func_torch.backward()\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{X.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "[[ 11731.5 -70389.   27373.5]\n",
      " [-15642.   62568.  -15642. ]\n",
      " [ 19552.5 -23463.    3910.5]]\n"
     ]
    }
   ],
   "source": [
    "# Definimos Nuestras Matrices\n",
    "\n",
    "X = jaxnp.array([[1.0, 2.0, 7.0], [3.0, 4.0, 9.0], [5.0, 2.0, 3.0]])\n",
    "A = jaxnp.array([[2.0, 1.0, 0.5], [1.0, 3.0, 1.0], [0.5, 1.0, 2.0]])\n",
    "B = jaxnp.array([[5.0, 4.0, 5.0], [8.0, 7.0, 2.0], [10.0, 1.0, 1.0]])\n",
    "\n",
    "# Definimos Nuestra Funcion Escalar\n",
    "\n",
    "def func_jax(X):\n",
    "    \n",
    "    return jaxnp.linalg.det(jaxnp.matmul(jaxnp.matmul(A, X), B))\n",
    "\n",
    "# Calculamos el Gradiente \n",
    "\n",
    "func_grad = grad(fun = func_jax)\n",
    "\n",
    "# Obtenemos el Gradiente con respecto a X \n",
    "\n",
    "X_grad = func_grad(X)\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{X_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gradiente Respecto a X: \n",
      "tensor([[ 11731.5010, -70389.0078,  27373.5000],\n",
      "        [-15642.0000,  62568.0000, -15642.0000],\n",
      "        [ 19552.4961, -23463.0000,   3910.5010]])\n"
     ]
    }
   ],
   "source": [
    "# Definimos Nuestra Matrices\n",
    "\n",
    "X = torch.tensor([[1.0, 2.0, 7.0], [3.0, 4.0, 9.0], [5.0, 2.0, 3.0]])\n",
    "A = torch.tensor([[2.0, 1.0, 0.5], [1.0, 3.0, 1.0], [0.5, 1.0, 2.0]])\n",
    "B = torch.tensor([[5.0, 4.0, 5.0], [8.0, 7.0, 2.0], [10.0, 1.0, 1.0]])\n",
    "\n",
    "# Calculamos el Gradiente\n",
    "\n",
    "gradient = torch.linalg.det(torch.matmul(torch.matmul(A, X), B)) * torch.linalg.inv(X).T\n",
    "\n",
    "# Visualizamos \n",
    "\n",
    "print(f'El Gradiente Respecto a X: \\n{gradient}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MatrixCalculus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
