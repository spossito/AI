{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d772b58",
   "metadata": {},
   "source": [
    "# **Librerias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "582eacc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy Version: 2.3.1\n",
      "CVXPY Version: 1.7.1\n",
      "Matplotlib Version: 3.10.3\n"
     ]
    }
   ],
   "source": [
    "# Importamos las Librerias \n",
    "\n",
    "import matplotlib\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Versiones \n",
    "\n",
    "print(f'Numpy Version: {np.__version__}')\n",
    "print(f'CVXPY Version: {cp.__version__}')\n",
    "print(f'Matplotlib Version: {matplotlib.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a128ee",
   "metadata": {},
   "source": [
    "# **Lasso**\n",
    "\n",
    "Conocemos que podemos calcular el parametro de **maximo a posteriori** $w$, como un problema de **optimizacion convexa** utilizando la funcion de **Negative Log-Verosimilitud**:\n",
    "\n",
    "$$NLL(w \\mid y, X, \\sigma^2, b)  = D \\log \\left(2b\\right) + \\frac{1}{b} ||w||_1 + \\frac{n}{2} \\log \\left(2\\pi\\sigma^2\\right) + \\frac{1}{2 \\sigma^2} \\cdot (y - Xw)^T (y - Xw)$$\n",
    "\n",
    "$$\\text{Loss} = \\frac{1}{b} ||w||_1 + \\frac{1}{2 \\sigma^2} \\cdot (y - Xw)^T (y - Xw)$$\n",
    "$$\\text{Loss} = \\frac{\\sigma^2}{b} ||w||_1 + \\frac{1}{2} (y - Xw)^T (y - Xw)$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{minimizar}_w \\quad & \\frac{1}{2} ||y - Xw||_2^2 + \\lambda ||w||_1\n",
    "\\end{align*}$$\n",
    "\n",
    "En donde \n",
    "\n",
    "$$\\lambda = \\frac{\\sigma^2}{b}$$\n",
    "\n",
    "* $\\sigma^2$ grande, tenemos mas confianza en el **prior** ya que los datos son ruidosos \n",
    "\n",
    "* $b$ grande, tenemos mas confianza en el **likehood** ya que el prior es muy laxo \n",
    "\n",
    "* $b$ pequeño, tenemos mas confianza en el **prior** ya que el prior es muy restrictivo\n",
    "\n",
    "El mismo problema se puede expresar como una **restricción sobre el conjunto de soluciones factibles**:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{minimizar} \\quad & \\frac{1}{2} ||y - Xw||_2^2  \\\\ \\text{sujeto a} \\quad & ||w||_1 - t \\leq 0\n",
    "\\end{align*}$$\n",
    "\n",
    "En donde \n",
    "\n",
    "* $t$ grande, tenemos mas confianza en el **likehood** ya que el prior es muy laxo \n",
    "\n",
    "* $t$ pequeño, tenemos mas confianza en el **prior** ya que el prior es muy restrictivo\n",
    "\n",
    "Otra forma de interpretarlo \n",
    "\n",
    "* $t$ grande, restricción débil, solución cercana a mínimos cuadrados (OLS).\n",
    "\n",
    "* $t$ pequeño, restricción fuerte, los pesos se contraen hacia 0, promoviendo sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da83da30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51606d62e96c4d349a5b4cb747b0f69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=102.57229603433198, description='t', max=102.57229603433198, min=10.25…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Semilla\n",
    "\n",
    "np.random.seed(25)\n",
    "\n",
    "# Definimos el Dataset\n",
    "\n",
    "X, y = make_regression(n_samples = 100, n_features = 2, noise = 10.0, random_state = 25)\n",
    "\n",
    "# Definimos la grilla de la Funcion de Perdidas (Minimos Cuadrados)\n",
    "\n",
    "w1_vals = np.linspace(-200, 400, 300)\n",
    "w2_vals = np.linspace(-200, 400, 300)\n",
    "\n",
    "W1, W2 = np.meshgrid(w1_vals, w2_vals)\n",
    "\n",
    "Loss = np.zeros_like(W1)\n",
    "\n",
    "for w1_i in range(0, W1.shape[0]):\n",
    "    for w2_i in range(0, W1.shape[1]):\n",
    "\n",
    "        w_i = np.array([W1[w1_i, w2_i], W2[w1_i, w2_i]])\n",
    "        b_i = np.mean(y - X @ w_i)\n",
    "\n",
    "        Loss[w1_i, w2_i] = np.sum((y - (X @ w_i + b_i))**2)\n",
    "\n",
    "# Calculamos el Punto Optimo de la Funcion de Perdidas sin Regularizacion (Minimos Cuadrados)\n",
    "\n",
    "model_least_square = LinearRegression()\n",
    "\n",
    "model_least_square.fit(X, y)\n",
    "\n",
    "w_opt_least_square = model_least_square.coef_\n",
    "b_opt_least_square = model_least_square.intercept_\n",
    "\n",
    "loss_least_square = np.sum((y - (X @ w_opt_least_square + b_opt_least_square))**2)\n",
    "\n",
    "# Funcion Interactiva que demuestra la Regularizacion \n",
    "\n",
    "def viz_lasso(t, elev = 30, azim = 45): \n",
    "    \n",
    "    X_with_bias = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "    weights = cp.Variable(3)\n",
    "    constraints = [cp.norm1(weights[1:]) <= t]\n",
    "    objective = cp.Minimize(0.5 * cp.sum_squares(y - X_with_bias @ weights))\n",
    "\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve()\n",
    "\n",
    "    w_opt_lasso = weights.value[1:]\n",
    "    b_opt_lasso = weights.value[0]\n",
    "\n",
    "    loss_lasso = np.sum((y - (X @ w_opt_lasso + b_opt_lasso))**2)\n",
    "\n",
    "    norm_w1 = np.array([0, t, 0, -t, 0])\n",
    "    norm_w2 = np.array([t, 0, -t, 0, t])\n",
    "\n",
    "    norm = np.zeros_like(norm_w1)\n",
    "    for idx in range(len(norm_w1)):\n",
    "        w_c = np.array([norm_w1[idx], norm_w2[idx]])\n",
    "        b_c = np.mean(y - X @ w_c)\n",
    "        norm[idx] = np.sum((y - (X @ w_c + b_c))**2)\n",
    "\n",
    "    fig = plt.figure(figsize = (10 , 8))\n",
    "    ax = fig.add_subplot(111, projection = '3d')\n",
    "    \n",
    "    ax.plot_surface(W1, W2, Loss, alpha = 0.5, cmap = 'inferno')\n",
    "    ax.plot(norm_w1, norm_w2, norm, color = 'blue', linewidth = 3, label = f'Norma L1 t = {t:.2f}')\n",
    "    ax.plot_trisurf(norm_w1, norm_w2, np.zeros_like(norm_w1), color = \"blue\", alpha = 0.3)\n",
    "    \n",
    "    ax.scatter(w_opt_lasso[0], w_opt_lasso[1], loss_lasso, color = 'black', s = 20, label = 'Óptimo Lasso')\n",
    "    ax.scatter(w_opt_least_square[0], w_opt_least_square[1], loss_least_square, color = 'green', s = 20, label = 'Óptimo Minimos Cuadrados')\n",
    "    \n",
    "    ax.view_init(elev = elev, azim = azim)    \n",
    "    ax.set_xlabel(r'$w_1$')\n",
    "    ax.set_ylabel(r'$w_2$')\n",
    "    ax.set_title(f'Loss Minimos Cuadrados {loss_least_square:.2f}, Loss Lasso {loss_lasso:.2f}\\nMinimos Cuadrados $\\\\rightarrow$ $w_1$ = {w_opt_least_square[0]:.2f} $w_2$ = {w_opt_least_square[1]:.2f}\\nLasso $\\\\rightarrow$ $w_1$ = {w_opt_lasso[0]:.2f} $w_2$ = {w_opt_lasso[1]:.2f}')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "t_max = np.linalg.norm(w_opt_least_square, ord = 1)\n",
    "\n",
    "t_slider = widgets.FloatSlider(value = t_max, min = 0.1*t_max, max = t_max, step = 0.05*t_max, description = 't')\n",
    "elev_slider = widgets.IntSlider(value = 30, min = 0, max = 90, step = 1, description = 'Elevacion')\n",
    "azim_slider = widgets.IntSlider(value = 45, min = 0, max = 360, step = 1, description = 'Azimut')\n",
    "\n",
    "widgets.interact(viz_lasso, t = t_slider, elev = elev_slider, azim = azim_slider)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8f5462",
   "metadata": {},
   "source": [
    "# **Ridge**\n",
    "\n",
    "Conocemos que podemos calcular el parametro de **maximo a posteriori** $w$, como un problema de **optimizacion convexa** utilizando la funcion de **Negative Log-Verosimilitud**:\n",
    "\n",
    "$$NLL(w \\mid y, X, \\sigma^2, \\tau^2) = \\frac{D}{2} \\log 2 \\pi \\tau^2 + \\frac{1}{2\\tau^2} w^Tw  + \\frac{n}{2} \\log 2\\pi\\sigma^2 + \\frac{1}{2 \\sigma^2} \\cdot (y - Xw)^T (y - Xw)$$\n",
    "\n",
    "$$\\text{Loss} = \\frac{1}{2\\tau^2} w^Tw + \\frac{1}{2 \\sigma^2} \\cdot (y - Xw)^T (y - Xw)$$\n",
    "$$\\text{Loss} = \\frac{\\sigma^2}{\\tau^2} w^Tw + (y - Xw)^T (y - Xw)$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{minimizar}_w \\quad & ||y - Xw||_2^2 + \\lambda ||w||_2^2 \n",
    "\\end{align*}$$\n",
    "\n",
    "En donde \n",
    "\n",
    "$$\\lambda = \\frac{\\sigma^2}{\\tau^2}$$\n",
    "\n",
    "* $\\sigma^2$ grande, tenemos mas confianza en el **prior** ya que los datos son ruidosos \n",
    "\n",
    "* $\\tau^2$ grande, tenemos mas confianza en el **likehood** ya que el prior es muy laxo \n",
    "\n",
    "* $\\tau^2$ pequeño, tenemos mas confianza en el **prior** ya que el prior es muy restrictivo\n",
    "\n",
    "El mismo problema se puede expresar como una **restricción sobre el conjunto de soluciones factibles**:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{minimizar} \\quad & ||y - Xw||_2^2  \\\\ \\text{sujeto a} \\quad & ||w||_2 - t \\leq 0\n",
    "\\end{align*}$$\n",
    "\n",
    "En donde \n",
    "\n",
    "* $t$ grande, tenemos mas confianza en el **likehood** ya que el prior es muy laxo \n",
    "\n",
    "* $t$ pequeño, tenemos mas confianza en el **prior** ya que el prior es muy restrictivo\n",
    "\n",
    "Otra forma de interpretarlo \n",
    "\n",
    "* $t$ grande, restricción débil, solución cercana a mínimos cuadrados (OLS).\n",
    "\n",
    "* $t$ pequeño, restricción fuerte, los pesos se contraen hacia valores pequeños, pero muy raramente hacia 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55a221af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2510a2458541ff838ae51d112f7d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=96.63766353897923, description='t', max=96.63766353897923, min=9.66376…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Semilla\n",
    "\n",
    "np.random.seed(25)\n",
    "\n",
    "# Definimos el Dataset\n",
    "\n",
    "X, y = make_regression(n_samples = 100, n_features = 2, noise = 10.0, random_state = 25)\n",
    "\n",
    "# Definimos la grilla de la Funcion de Perdidas (Minimos Cuadrados)\n",
    "\n",
    "w1_vals = np.linspace(-200, 400, 300)\n",
    "w2_vals = np.linspace(-200, 400, 300)\n",
    "\n",
    "W1, W2 = np.meshgrid(w1_vals, w2_vals)\n",
    "\n",
    "Loss = np.zeros_like(W1)\n",
    "\n",
    "for w1_i in range(0, W1.shape[0]):\n",
    "    for w2_i in range(0, W1.shape[1]):\n",
    "\n",
    "        w_i = np.array([W1[w1_i, w2_i], W2[w1_i, w2_i]])\n",
    "        b_i = np.mean(y - X @ w_i)\n",
    "\n",
    "        Loss[w1_i, w2_i] = np.sum((y - (X @ w_i + b_i))**2)\n",
    "\n",
    "# Calculamos el Punto Optimo de la Funcion de Perdidas sin Regularizacion (Minimos Cuadrados)\n",
    "\n",
    "model_least_square = LinearRegression()\n",
    "\n",
    "model_least_square.fit(X, y)\n",
    "\n",
    "w_opt_least_square = model_least_square.coef_\n",
    "b_opt_least_square = model_least_square.intercept_\n",
    "\n",
    "loss_least_square = np.sum((y - (X @ w_opt_least_square + b_opt_least_square))**2)\n",
    "\n",
    "# Funcion Interactiva que demuestra la Regularizacion \n",
    "\n",
    "def viz_ridge(t, elev = 30, azim = 45): \n",
    "    \n",
    "    X_with_bias = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "    weights = cp.Variable(3)\n",
    "    constraints = [cp.norm2(weights[1:]) <= t]\n",
    "    objective = cp.Minimize(cp.sum_squares(y - X_with_bias @ weights))\n",
    "\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve()\n",
    "\n",
    "    w_opt_ridge = weights.value[1:]\n",
    "    b_opt_ridge = weights.value[0]\n",
    "\n",
    "    loss_ridge = np.sum((y - (X @ w_opt_ridge + b_opt_ridge))**2)\n",
    "\n",
    "    theta = np.linspace(0, 2*np.pi, 200)\n",
    "    norm_w1 = t * np.cos(theta)\n",
    "    norm_w2 = t * np.sin(theta)\n",
    "\n",
    "    norm = np.zeros_like(norm_w1)\n",
    "    for idx in range(len(norm_w1)):\n",
    "        w_c = np.array([norm_w1[idx], norm_w2[idx]])\n",
    "        b_c = np.mean(y - X @ w_c)\n",
    "        norm[idx] = np.sum((y - (X @ w_c + b_c))**2)\n",
    "\n",
    "    fig = plt.figure(figsize = (10 , 8))\n",
    "    ax = fig.add_subplot(111, projection = '3d')\n",
    "    \n",
    "    ax.plot_surface(W1, W2, Loss, alpha = 0.5, cmap = 'inferno')\n",
    "    ax.plot(norm_w1, norm_w2, norm, color = 'blue', linewidth = 3, label = f'Norma L2 t = {t:.2f}')\n",
    "    ax.plot_trisurf(norm_w1, norm_w2, np.zeros_like(norm_w1), color = \"blue\", alpha = 0.3)\n",
    "    \n",
    "    ax.scatter(w_opt_ridge[0], w_opt_ridge[1], loss_ridge, color = 'black', s = 20, label = 'Óptimo Ridge')\n",
    "    ax.scatter(w_opt_least_square[0], w_opt_least_square[1], loss_least_square, color = 'green', s = 20, label = 'Óptimo Minimos Cuadrados')\n",
    "    \n",
    "    ax.view_init(elev = elev, azim = azim)    \n",
    "    ax.set_xlabel(r'$w_1$')\n",
    "    ax.set_ylabel(r'$w_2$')\n",
    "    ax.set_title(f'Loss Minimos Cuadrados {loss_least_square:.2f}, Loss Ridge {loss_ridge:.2f}\\nMinimos Cuadrados $\\\\rightarrow$ $w_1$ = {w_opt_least_square[0]:.2f} $w_2$ = {w_opt_least_square[1]:.2f}\\nRidge $\\\\rightarrow$ $w_1$ = {w_opt_ridge[0]:.2f} $w_2$ = {w_opt_ridge[1]:.2f}')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "t_max = np.linalg.norm(w_opt_least_square, ord = 2)\n",
    "\n",
    "t_slider = widgets.FloatSlider(value = t_max, min = 0.1*t_max, max = t_max, step = 0.05*t_max, description = 't')\n",
    "elev_slider = widgets.IntSlider(value = 30, min = 0, max = 90, step = 1, description = 'Elevacion')\n",
    "azim_slider = widgets.IntSlider(value = 45, min = 0, max = 360, step = 1, description = 'Azimut')\n",
    "\n",
    "widgets.interact(viz_ridge, t = t_slider, elev = elev_slider, azim = azim_slider)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
